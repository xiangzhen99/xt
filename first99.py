import streamlit as st
import pandas as pd
import numpy as np
import pickle
import os
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.impute import SimpleImputer

# ----------------------1. å…¨å±€é…ç½®ä¸åˆå§‹åŒ–----------------------
# é¡µé¢åŸºç¡€è®¾ç½®
st.set_page_config(
    page_title="ä¼é¹…åˆ†ç±»å™¨",
    page_icon=":penguin:",
    layout="wide"
)

# å®šä¹‰æ–‡ä»¶è·¯å¾„ï¼ˆç¡®ä¿æ•°æ®é›†ä¸ä»£ç åŒç›®å½•ï¼‰
DATA_PATH = "penguins-chinese.csv"
MODEL_PATH = "rfc_model.pkl"
MAP_PATH = "output_uniques.pkl"
FEATURE_PATH = "feature_names.pkl"

# å›¾ç‰‡è·¯å¾„ï¼ˆéœ€åœ¨ä»£ç åŒç›®å½•åˆ›å»ºimagesæ–‡ä»¶å¤¹å¹¶æ”¾å…¥å¯¹åº”å›¾ç‰‡ï¼‰
LOGO_PATH = "images/rigth_logo.png"
PENGUINS_SUMMARY_PATH = "images/penguins.png"
SPECIES_IMAGE_PATHS = {
    "é˜¿å¾·åˆ©ä¼é¹…": "images/é˜¿å¾·åˆ©ä¼é¹….png",
    "å·´å¸ƒäºšä¼é¹…": "images/å·´å¸ƒäºšä¼é¹….png",
    "å¸½å¸¦ä¼é¹…": "images/å¸½å¸¦ä¼é¹….png"
}

# ----------------------2. æ•°æ®é¢„å¤„ç†ä¸æ¨¡å‹è®­ç»ƒå·¥å…·å‡½æ•°----------------------
def preprocess_data(df):
    """æ•°æ®é¢„å¤„ç†ï¼šå¤„ç†ç¼ºå¤±å€¼ã€ç¼–ç åˆ†ç±»ç‰¹å¾"""
    # 1. ä¿®å¤åˆ—åä¸æ•°æ®é”™ä½ï¼ˆåŸæ•°æ®"å–™çš„é•¿åº¦"åˆ—æ··å…¥æ€§åˆ«æ•°æ®ï¼Œé‡æ–°å®šä¹‰æœ‰æ•ˆåˆ—ï¼‰
    correct_columns = ["ä¼é¹…çš„ç§ç±»", "ä¼é¹…æ –æ¯çš„å²›å±¿", "æ€§åˆ«", "å–™çš„æ·±åº¦", "ç¿…è†€çš„é•¿åº¦", "èº«ä½“è´¨é‡"]
    df_clean = df[correct_columns].copy()

    # 2. å¤„ç†ç¼ºå¤±å€¼
    # æ•°å€¼ç‰¹å¾ï¼ˆå–™çš„æ·±åº¦ã€ç¿…è†€çš„é•¿åº¦ã€èº«ä½“è´¨é‡ï¼‰ç”¨ä¸­ä½æ•°å¡«å……
    num_features = ["å–™çš„æ·±åº¦", "ç¿…è†€çš„é•¿åº¦", "èº«ä½“è´¨é‡"]
    num_imputer = SimpleImputer(strategy="median")
    df_clean[num_features] = num_imputer.fit_transform(df_clean[num_features])

    # åˆ†ç±»ç‰¹å¾ï¼ˆå²›å±¿ã€æ€§åˆ«ï¼‰ç”¨ä¼—æ•°å¡«å……
    cat_features = ["ä¼é¹…æ –æ¯çš„å²›å±¿", "æ€§åˆ«"]
    cat_imputer = SimpleImputer(strategy="most_frequent")
    df_clean[cat_features] = cat_imputer.fit_transform(df_clean[cat_features])

    # 3. ç¼–ç åˆ†ç±»ç‰¹å¾
    # å²›å±¿ï¼šOne-Hotç¼–ç ï¼ˆdrop='first'é¿å…å¤šé‡å…±çº¿æ€§ï¼‰
    encoder_island = OneHotEncoder(sparse_output=False, drop="first")
    island_encoded = encoder_island.fit_transform(df_clean[["ä¼é¹…æ –æ¯çš„å²›å±¿"]])
    island_df = pd.DataFrame(
        island_encoded,
        columns=[f"å²›å±¿_{cat}" for cat in encoder_island.categories_[0][1:]]  # åˆ—åï¼šå²›å±¿_æ¯”æ–¯ç§‘ç¾¤å²›ã€å²›å±¿_å¾·é‡Œå§†å²›
    )

    # æ€§åˆ«ï¼šLabelç¼–ç ï¼ˆé›Œæ€§=0ï¼Œé›„æ€§=1ï¼‰
    encoder_sex = LabelEncoder()
    df_clean["æ€§åˆ«_ç¼–ç "] = encoder_sex.fit_transform(df_clean["æ€§åˆ«"])

    # 4. ç¼–ç ç›®æ ‡å˜é‡ï¼ˆä¼é¹…ç§ç±»ï¼‰
    label_encoder_species = LabelEncoder()
    df_clean["ç‰©ç§_ç¼–ç "] = label_encoder_species.fit_transform(df_clean["ä¼é¹…çš„ç§ç±»"])

    # 5. æ„å»ºç‰¹å¾çŸ©é˜µXå’Œç›®æ ‡å˜é‡y
    X = pd.concat([
        df_clean[num_features],
        island_df,
        df_clean[["æ€§åˆ«_ç¼–ç "]]
    ], axis=1)
    y = df_clean["ç‰©ç§_ç¼–ç "]

    # å®šä¹‰ç‰¹å¾åï¼ˆç”¨äºåç»­è¾“å…¥åŒ¹é…ï¼‰
    feature_names = num_features + list(island_df.columns) + ["æ€§åˆ«_ç¼–ç "]
    X.columns = feature_names

    # è¿”å›é¢„å¤„ç†ç»“æœä¸ç¼–ç å™¨
    return X, y, feature_names, label_encoder_species, encoder_island, encoder_sex

def train_and_save_model():
    """è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹å¹¶ä¿å­˜ï¼ˆé¦–æ¬¡è¿è¡Œæˆ–æ¨¡å‹æ–‡ä»¶ç¼ºå¤±æ—¶è°ƒç”¨ï¼‰"""
    # 1. åŠ è½½æ•°æ®é›†ï¼ˆå…³é”®æ”¹åŠ¨ï¼šæŒ‡å®šç¼–ç ä¸º'gbk'ï¼Œè§£å†³UnicodeDecodeErrorï¼‰
    if not os.path.exists(DATA_PATH):
        st.error(f"æ•°æ®é›†æ–‡ä»¶æœªæ‰¾åˆ°ï¼è¯·ç¡®ä¿{DATA_PATH}ä¸ä»£ç åœ¨åŒä¸€ç›®å½•")
        st.stop()
    df = pd.read_csv(DATA_PATH, encoding='gbk')

    # 2. æ•°æ®é¢„å¤„ç†
    X, y, feature_names, label_encoder_species, _, _ = preprocess_data(df)

    # 3. åˆ’åˆ†è®­ç»ƒé›†ä¸æµ‹è¯•é›†
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # 4. è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹
    rfc_model = RandomForestClassifier(n_estimators=100, random_state=42)
    rfc_model.fit(X_train, y_train)

    # 5. éªŒè¯æ¨¡å‹ï¼ˆæ˜¾ç¤ºå‡†ç¡®ç‡ï¼‰
    train_acc = rfc_model.score(X_train, y_train)
    test_acc = rfc_model.score(X_test, y_test)
    st.success(f"æ¨¡å‹è®­ç»ƒå®Œæˆï¼è®­ç»ƒå‡†ç¡®ç‡ï¼š{train_acc:.2f}ï¼Œæµ‹è¯•å‡†ç¡®ç‡ï¼š{test_acc:.2f}")

    # 6. ä¿å­˜æ¨¡å‹ã€ç‰¹å¾åä¸ç‰©ç§æ˜ å°„
    with open(MODEL_PATH, "wb") as f:
        pickle.dump(rfc_model, f)
    
    with open(FEATURE_PATH, "wb") as f:
        pickle.dump(feature_names, f)
    
    # ç‰©ç§ç¼–ç æ˜ å°„ï¼ˆæ•°å­—â†’ä¸­æ–‡åç§°ï¼‰
    species_map = dict(zip(
        label_encoder_species.transform(label_encoder_species.classes_),
        label_encoder_species.classes_
    ))
    # é€‚é…åŸä»£ç ä¸­çš„æ˜ å°„æ ¼å¼ï¼ˆ{0: [ç‰©ç§å], 1: [ç‰©ç§å], ...}ï¼‰
    output_uniques_map = {k: [v] for k, v in species_map.items()}
    with open(MAP_PATH, "wb") as f:
        pickle.dump(output_uniques_map, f)

    return rfc_model, output_uniques_map, feature_names

def load_model_or_train():
    """åŠ è½½å·²ä¿å­˜çš„æ¨¡å‹ï¼Œè‹¥æ¨¡å‹ä¸å­˜åœ¨åˆ™è‡ªåŠ¨è®­ç»ƒ"""
    if os.path.exists(MODEL_PATH) and os.path.exists(MAP_PATH) and os.path.exists(FEATURE_PATH):
        # åŠ è½½å·²ä¿å­˜çš„æ¨¡å‹ä¸æ˜ å°„
        with open(MODEL_PATH, "rb") as f:
            rfc_model = pickle.load(f)
        with open(MAP_PATH, "rb") as f:
            output_uniques_map = pickle.load(f)
        with open(FEATURE_PATH, "rb") as f:
            feature_names = pickle.load(f)
        return rfc_model, output_uniques_map, feature_names
    else:
        # æ¨¡å‹ä¸å­˜åœ¨ï¼Œè‡ªåŠ¨è®­ç»ƒ
        st.info("æœªæ£€æµ‹åˆ°é¢„è®­ç»ƒæ¨¡å‹ï¼Œæ­£åœ¨è‡ªåŠ¨è®­ç»ƒ...")
        return train_and_save_model()

# ----------------------3. ç”¨æˆ·è¾“å…¥å¤„ç†å‡½æ•°ï¼ˆæ ¸å¿ƒä¿®å¤ï¼šä¸¥æ ¼åŒ¹é…ç‰¹å¾åç§°ä¸ç¼–ç ï¼‰----------------------
def process_user_input(island, sex, bill_depth, flipper_length, body_mass, feature_names):
    """å°†ç”¨æˆ·è¾“å…¥è½¬æ¢ä¸ºæ¨¡å‹å¯æ¥å—çš„ç‰¹å¾æ ¼å¼ï¼ˆç¡®ä¿ç‰¹å¾åç§°ä¸æ¨¡å‹è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼‰"""
    # 1. ç¼–ç å²›å±¿ï¼ˆä¸è®­ç»ƒæ—¶çš„One-Hotç¼–ç å®Œå…¨å¯¹é½ï¼‰
    island_biscoe = 1 if island == "æ¯”æ–¯ç§‘ç¾¤å²›" else 0
    island_dream = 1 if island == "å¾·é‡Œå§†å²›" else 0
    # æ³¨æ„ï¼šæ‰˜å°”æ£®å²›æ˜¯One-Hotç¼–ç çš„åŸºå‡†ç±»åˆ«ï¼ˆdrop='first'æ—¶è¢«æ’é™¤ï¼‰ï¼Œæ— éœ€æ˜¾å¼ç¼–ç 

    # 2. ç¼–ç æ€§åˆ«ï¼ˆé›Œæ€§=0ï¼Œé›„æ€§=1ï¼‰
    sex_encoded = 1 if sex == "é›„æ€§" else 0

    # 3. æ„å»ºç‰¹å¾åˆ—è¡¨ï¼ˆä¸¥æ ¼åŒ¹é…æ¨¡å‹è®­ç»ƒæ—¶çš„ç‰¹å¾é¡ºåºå’Œåç§°ï¼‰
    input_data = [
        bill_depth,          # å–™çš„æ·±åº¦
        flipper_length,      # ç¿…è†€çš„é•¿åº¦
        body_mass,           # èº«ä½“è´¨é‡
        island_biscoe,       # å²›å±¿_æ¯”æ–¯ç§‘ç¾¤å²›
        island_dream,        # å²›å±¿_å¾·é‡Œå§†å²›
        sex_encoded          # æ€§åˆ«_ç¼–ç 
    ]

    # 4. è½¬æ¢ä¸ºDataFrameï¼ˆç¡®ä¿ç‰¹å¾åä¸æ¨¡å‹è®­ç»ƒæ—¶å®Œå…¨åŒ¹é…ï¼‰
    input_df = pd.DataFrame([input_data], columns=feature_names)
    return input_df

# ----------------------4. é¡µé¢å†…å®¹ä¸äº¤äº’é€»è¾‘----------------------
def main():
    # åŠ è½½æ¨¡å‹ï¼ˆé¦–æ¬¡è¿è¡Œè‡ªåŠ¨è®­ç»ƒï¼‰
    rfc_model, output_uniques_map, feature_names = load_model_or_train()

    # ä¾§è¾¹æ é¡µé¢é€‰æ‹©
    with st.sidebar:
        # æ˜¾ç¤ºä¾§è¾¹æ Logoï¼ˆè‹¥æ–‡ä»¶ä¸å­˜åœ¨æ˜¾ç¤ºæ–‡æœ¬æç¤ºï¼‰
        if os.path.exists(LOGO_PATH):
            st.image(LOGO_PATH, width=100)
        else:
            st.write("ğŸ” è¯·åœ¨imagesæ–‡ä»¶å¤¹æ”¾å…¥rigth_logo.png")
        
        st.title("è¯·é€‰æ‹©é¡µé¢")
        page = st.selectbox(
            label="é¡µé¢é€‰æ‹©",
            options=["ç®€ä»‹é¡µé¢", "é¢„æµ‹åˆ†ç±»é¡µé¢"],
            label_visibility="collapsed"
        )

    # ----------------------5. ç®€ä»‹é¡µé¢----------------------
    if page == "ç®€ä»‹é¡µé¢":
        st.title("ä¼é¹…åˆ†ç±»å™¨ :penguin:")

        # æ•°æ®é›†ä»‹ç»
        st.header("ä¸€ã€æ•°æ®é›†ä»‹ç»")
        st.markdown("""
        å¸•å°”é»˜ç¾¤å²›ä¼é¹…æ•°æ®é›†æ˜¯æ•°æ®ç§‘å­¦å…¥é—¨çš„ç»å…¸æ•°æ®é›†ï¼Œç”±Gormanç­‰äººæ”¶é›†å¹¶å‘å¸ƒï¼ŒåŒ…å«**344æ¡è§‚æµ‹è®°å½•**ï¼Œæ¶µç›–å—æ3ç§ä¼é¹…çš„æ ¸å¿ƒç”Ÿç‰©å­¦ç‰¹å¾ï¼š
        - **é˜¿å¾·åˆ©ä¼é¹…**ï¼šä½“å‹è¾ƒå°ï¼Œå¹¿æ³›åˆ†å¸ƒäºæ‰˜å°”æ£®å²›ã€æ¯”æ–¯ç§‘ç¾¤å²›ã€å¾·é‡Œå§†å²›
        - **å·´å¸ƒäºšä¼é¹…**ï¼šä½“å‹è¾ƒå¤§ï¼Œä¸»è¦æ –æ¯äºæ¯”æ–¯ç§‘ç¾¤å²›
        - **å¸½å¸¦ä¼é¹…**ï¼šé¢ˆéƒ¨æœ‰é»‘è‰²"å¸½å¸¦"çº¹è·¯ï¼Œä»…æ –æ¯äºå¾·é‡Œå§†å²›
        
        æ•°æ®é›†æ ¸å¿ƒç‰¹å¾åŒ…æ‹¬ï¼šä¼é¹…æ –æ¯çš„å²›å±¿ã€æ€§åˆ«ã€å–™çš„æ·±åº¦ï¼ˆæ¯«ç±³ï¼‰ã€ç¿…è†€çš„é•¿åº¦ï¼ˆæ¯«ç±³ï¼‰ã€èº«ä½“è´¨é‡ï¼ˆå…‹ï¼‰ï¼Œå¯ç”¨äºç‰©ç§åˆ†ç±»ã€ç‰¹å¾ç›¸å…³æ€§åˆ†æç­‰ä»»åŠ¡ã€‚
        """)

        # ä¸‰ç§ä¼é¹…å›¾ç‰‡å±•ç¤º
        st.header("äºŒã€ä¸‰ç§ä¼é¹…å¯¹æ¯”å›¾")
        if os.path.exists(PENGUINS_SUMMARY_PATH):
            st.image(PENGUINS_SUMMARY_PATH, caption="å¸•å°”é»˜ç¾¤å²›ä¸‰ç§ä¼é¹…å¡é€šå›¾")
        else:
            st.warning("âš ï¸ æœªæ‰¾åˆ°ä¸‰ç§ä¼é¹…æ±‡æ€»å›¾ï¼ˆimages/penguins.pngï¼‰ï¼Œè¯·è¡¥å……å›¾ç‰‡æ–‡ä»¶")

    # ----------------------6. é¢„æµ‹åˆ†ç±»é¡µé¢----------------------
    elif page == "é¢„æµ‹åˆ†ç±»é¡µé¢":
        st.header("é¢„æµ‹ä¼é¹…åˆ†ç±»")
        st.markdown("""
        åŸºäºéšæœºæ£®æ—æ¨¡å‹çš„ä¼é¹…ç‰©ç§é¢„æµ‹å·¥å…·ï¼  
        è¯·è¾“å…¥ä»¥ä¸‹ç‰¹å¾ï¼ˆæ•°å€¼å‚è€ƒå®é™…èŒƒå›´ï¼šå–™æ·±åº¦13-22mmï¼Œç¿…è†€é•¿åº¦170-230mmï¼Œèº«ä½“è´¨é‡2700-6300gï¼‰ï¼Œç‚¹å‡»"é¢„æµ‹åˆ†ç±»"è·å–ç»“æœã€‚
        """)

        # é¡µé¢å¸ƒå±€ï¼š3åˆ—ï¼ˆè¡¨å•åˆ—:é—´éš”åˆ—:ç»“æœå±•ç¤ºåˆ—ï¼‰
        col_form, col_space, col_result = st.columns([3, 1, 2])

        # å·¦ä¾§ï¼šç”¨æˆ·è¾“å…¥è¡¨å•
        with col_form:
            with st.form("user_input_form"):
                # 1. é€‰æ‹©å²›å±¿ï¼ˆä¸æ¨¡å‹è®­ç»ƒæ—¶çš„ç±»åˆ«å®Œå…¨ä¸€è‡´ï¼‰
                island = st.selectbox(
                    label="1. ä¼é¹…æ –æ¯çš„å²›å±¿",
                    options=["æ‰˜å°”æ£®å²›", "æ¯”æ–¯ç§‘ç¾¤å²›", "å¾·é‡Œå§†å²›"]
                )

                # 2. é€‰æ‹©æ€§åˆ«ï¼ˆä¸æ¨¡å‹è®­ç»ƒæ—¶çš„ç±»åˆ«å®Œå…¨ä¸€è‡´ï¼‰
                sex = st.selectbox(
                    label="2. æ€§åˆ«",
                    options=["é›Œæ€§", "é›„æ€§"]
                )

                # 3. å–™çš„æ·±åº¦ï¼ˆæ¯«ç±³ï¼‰
                bill_depth = st.number_input(
                    label="3. å–™çš„æ·±åº¦ï¼ˆæ¯«ç±³ï¼‰",
                    min_value=13.0,
                    max_value=22.0,
                    step=0.1,
                    value=18.0,
                    help="åˆç†èŒƒå›´ï¼š13.0-22.0æ¯«ç±³"
                )

                # 4. ç¿…è†€çš„é•¿åº¦ï¼ˆæ¯«ç±³ï¼‰
                flipper_length = st.number_input(
                    label="4. ç¿…è†€çš„é•¿åº¦ï¼ˆæ¯«ç±³ï¼‰",
                    min_value=170.0,
                    max_value=230.0,
                    step=1.0,
                    value=190.0,
                    help="åˆç†èŒƒå›´ï¼š170.0-230.0æ¯«ç±³"
                )

                # 5. èº«ä½“è´¨é‡ï¼ˆå…‹ï¼‰
                body_mass = st.number_input(
                    label="5. èº«ä½“è´¨é‡ï¼ˆå…‹ï¼‰",
                    min_value=2700.0,
                    max_value=6300.0,
                    step=50.0,
                    value=3500.0,
                    help="åˆç†èŒƒå›´ï¼š2700.0-6300.0å…‹"
                )

                # æäº¤æŒ‰é’®
                submitted = st.form_submit_button("é¢„æµ‹åˆ†ç±»", type="primary")

        # å³ä¾§ï¼šç»“æœå±•ç¤ºï¼ˆæœªæäº¤æ—¶æ˜¾ç¤ºLogoï¼Œæäº¤åæ˜¾ç¤ºé¢„æµ‹ç»“æœï¼‰
        with col_result:
            if not submitted:
                # æœªæäº¤æ—¶æ˜¾ç¤ºé»˜è®¤Logo
                if os.path.exists(LOGO_PATH):
                    st.image(LOGO_PATH, width=300, caption="è¯·è¾“å…¥ç‰¹å¾å¹¶ç‚¹å‡»é¢„æµ‹")
                else:
                    st.write("ğŸ” ç­‰å¾…è¾“å…¥ç‰¹å¾...")
            else:
                # å¤„ç†ç”¨æˆ·è¾“å…¥ï¼ˆç¡®ä¿ç‰¹å¾ç¼–ç ä¸æ¨¡å‹è®­ç»ƒæ—¶å®Œå…¨å¯¹é½ï¼‰
                input_df = process_user_input(island, sex, bill_depth, flipper_length, body_mass, feature_names)
                
                # æ¨¡å‹é¢„æµ‹
                predict_code = rfc_model.predict(input_df)[0]  # è·å–é¢„æµ‹ç¼–ç 
                predict_species = output_uniques_map[predict_code][0]  # æ˜ å°„ä¸ºä¸­æ–‡ç‰©ç§å

                # æ˜¾ç¤ºé¢„æµ‹ç»“æœ
                st.success("âœ… é¢„æµ‹å®Œæˆï¼")
                st.write(f"### é¢„æµ‹ç‰©ç§ï¼š**{predict_species}**")

                # æ˜¾ç¤ºå¯¹åº”ç‰©ç§å›¾ç‰‡
                species_img_path = SPECIES_IMAGE_PATHS.get(predict_species)
                if os.path.exists(species_img_path):
                    st.image(species_img_path, width=300, caption=f"{predict_species}ç‰¹å¾å›¾")
                else:
                    st.warning(f"âš ï¸ æœªæ‰¾åˆ°{predict_species}å›¾ç‰‡ï¼Œè¯·è¡¥å……{species_img_path}æ–‡ä»¶")

                # æ˜¾ç¤ºç‰©ç§å°çŸ¥è¯†
                species_knowledge = {
                    "é˜¿å¾·åˆ©ä¼é¹…": "ğŸ“Œ å°çŸ¥è¯†ï¼šé˜¿å¾·åˆ©ä¼é¹…æ˜¯å—ææœ€å¸¸è§çš„ä¼é¹…ï¼Œæ“…é•¿åœ¨å†°é¢å¿«é€Ÿè¡Œèµ°ï¼Œä»¥ç£·è™¾å’Œé±¼ç±»ä¸ºé£Ÿã€‚",
                    "å·´å¸ƒäºšä¼é¹…": "ğŸ“Œ å°çŸ¥è¯†ï¼šå·´å¸ƒäºšä¼é¹…åˆç§°'ç»…å£«ä¼é¹…'ï¼Œæ¸¸æ³³é€Ÿåº¦å¯è¾¾36å…¬é‡Œ/å°æ—¶ï¼Œæ˜¯ä¼é¹…ä¸­æ¸¸æ³³æœ€å¿«çš„ç‰©ç§ä¹‹ä¸€ã€‚",
                    "å¸½å¸¦ä¼é¹…": "ğŸ“Œ å°çŸ¥è¯†ï¼šå¸½å¸¦ä¼é¹…å› é¢ˆéƒ¨çš„é»‘è‰²æ¡çº¹å½¢ä¼¼å¸½å¸¦å¾—åï¼Œæ€§æ ¼è¾ƒå‡¶çŒ›ï¼Œå¸¸ä¸å…¶ä»–ä¼é¹…äº‰å¤ºæ –æ¯åœ°ã€‚"
                }
                st.markdown(species_knowledge.get(predict_species, ""))

# ----------------------7. å¯åŠ¨åº”ç”¨----------------------
if __name__ == "__main__":
    main()
